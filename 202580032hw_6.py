# -*- coding: utf-8 -*-
"""202580032hW_6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z0l89_ORe_NZttmCuBnR95G9q_eFga1Z
"""

# IMDB Movie Review Sentiment Classification (SimpleRNN)


import time
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

print("TensorFlow version:", tf.__version__)

# PARAMETERS

vocab_size = 10000     # Top 10k words
maxlen = 500           # Max review length
embedding_dim = 128
batch_size = 32
epochs = 10


# LOAD IMDB DATASET

print("\nLoading IMDB dataset...")
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)
print("Training samples:", len(x_train))
print("Testing samples:", len(x_test))

# Pad sequences to fixed length
x_train = pad_sequences(x_train, maxlen=maxlen)
x_test  = pad_sequences(x_test,  maxlen=maxlen)

print(f"x_train shape: {x_train.shape}")
print(f"x_test shape: {x_test.shape}")


# THE MODEL - SimpleRNN with 2 RNN layers and 2 dense layers

model = keras.Sequential([
    # Embedding layer - FIXED: removed batch_input_shape
    layers.Embedding(vocab_size, embedding_dim, input_length=maxlen, name="embedding_1"),

    # First SimpleRNN layer
    layers.SimpleRNN(64, dropout=0.2, return_sequences=True, name="simple_rnn_1"),

    # Second SimpleRNN layer
    layers.SimpleRNN(32, dropout=0.2, name="simple_rnn_2"),

    # First Dense layer
    layers.Dense(32, activation="relu", name="dense_1"),
    layers.Dropout(0.3),

    # Second Dense layer
    layers.Dense(16, activation="relu", name="dense_2"),
    layers.Dropout(0.3),

    # Output layer
    layers.Dense(1, activation="sigmoid", name="output")
])

model.compile(
    optimizer="adam",
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

print("\nBuilding model...")
# Create a sample batch to build the model
sample_batch = x_train[:batch_size]
_ = model(sample_batch)


# SHOW MODEL SUMMARY

print("\n" + "="*60)
print("MODEL SUMMARY")
print("="*60)
model.summary()

# TRAINING WITH TIMER

print("\n" + "="*60)
print("TRAINING PROCESS")
print("="*60)

start_time = time.time()

history = model.fit(
    x_train,
    y_train,
    epochs=epochs,
    batch_size=batch_size,
    validation_data=(x_test, y_test),
    verbose=1
)

end_time = time.time()
training_time = end_time - start_time

print(f"\nTraining Finished. Total Time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)")

# EVALUATE THE MODEL

print("\n" + "="*60)
print("FINAL RESULTS")
print("="*60)

test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_acc:.4f}")

# SHOW TRAINING HISTORY AS A TABLE

df_hist = pd.DataFrame(history.history)
df_hist.index = [f"epoch_{i+1}" for i in range(len(df_hist))]

print("\n" + "="*60)
print("TRAINING HISTORY")
print("="*60)
print(df_hist.round(4))

# ------------------------------------------------------------
# PLOT TRAINING HISTORY
# ------------------------------------------------------------
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss', marker='o')
plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
plt.show()


# SAVE THE MODEL

model.save('my_movie_review_rnn.h5')
print("\nModel saved as 'my_movie_review_rnn.h5'")

print("\n" + "="*60)
print("PROGRAM COMPLETED SUCCESSFULLY")
print("="*60)

